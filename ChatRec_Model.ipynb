{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rEP-oFD6Lqh"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers pandas numpy scikit-learn nltk rouge-score tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import nltk; nltk.download('punkt'); nltk.download('wordnet')\""
      ],
      "metadata": {
        "id": "xyJXnkcs6W_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Conversational Response Prediction System\n",
        "==========================================\n",
        "A complete pipeline for predicting User A's responses based on conversation history.\n",
        "\n",
        "This system:\n",
        "1. Preprocesses and tokenizes conversational data\n",
        "2. Fine-tunes a GPT-2 model for response generation\n",
        "3. Generates context-aware replies\n",
        "4. Evaluates using BLEU, ROUGE, and Perplexity metrics\n",
        "5. Provides deployment-ready components\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    GPT2Config,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For evaluation metrics\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import math\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: DATA PREPROCESSING AND TOKENIZATION\n",
        "# ============================================================================\n",
        "\n",
        "class ConversationDataProcessor:\n",
        "    \"\"\"Handles data loading, preprocessing, and separation of user conversations.\"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "        \"\"\"\n",
        "        Initialize with conversation data.\n",
        "\n",
        "        Args:\n",
        "            data: DataFrame or path to CSV/PDF containing conversation data\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.user_a_conversations = []\n",
        "        self.user_b_conversations = []\n",
        "\n",
        "    def load_and_parse_data(self):\n",
        "        \"\"\"Parse the conversation data and separate by user.\"\"\"\n",
        "        if isinstance(self.data, str):\n",
        "            # If data is a file path\n",
        "            if self.data.endswith('.csv'):\n",
        "                df = pd.read_csv(self.data)\n",
        "            else:\n",
        "                # Assuming PDF export of CSV\n",
        "                df = pd.read_csv(self.data)\n",
        "        else:\n",
        "            df = self.data\n",
        "\n",
        "        # Group by conversation ID\n",
        "        conversations = df.groupby('Conversation ID')\n",
        "\n",
        "        processed_conversations = []\n",
        "\n",
        "        for conv_id, conv_data in conversations:\n",
        "            conv_data = conv_data.sort_values('Timestamp')\n",
        "\n",
        "            messages = []\n",
        "            for _, row in conv_data.iterrows():\n",
        "                messages.append({\n",
        "                    'sender': row['Sender'],\n",
        "                    'message': row['Message'],\n",
        "                    'timestamp': row['Timestamp']\n",
        "                })\n",
        "\n",
        "            processed_conversations.append({\n",
        "                'conversation_id': conv_id,\n",
        "                'messages': messages\n",
        "            })\n",
        "\n",
        "        return processed_conversations\n",
        "\n",
        "    def clean_message(self, message):\n",
        "        \"\"\"Clean and normalize message text.\"\"\"\n",
        "        # Remove quotes if present\n",
        "        message = message.strip('\"\\'')\n",
        "        # Normalize whitespace\n",
        "        message = re.sub(r'\\s+', ' ', message)\n",
        "        return message.strip()\n",
        "\n",
        "    def create_training_pairs(self, conversations):\n",
        "        \"\"\"\n",
        "        Create (context, response) pairs for training.\n",
        "        Context includes conversation history, response is User A's reply.\n",
        "        \"\"\"\n",
        "        training_pairs = []\n",
        "\n",
        "        for conv in conversations:\n",
        "            messages = conv['messages']\n",
        "            context_window = []\n",
        "\n",
        "            for i, msg in enumerate(messages):\n",
        "                sender = msg['sender']\n",
        "                message = self.clean_message(msg['message'])\n",
        "\n",
        "                if sender == 'User A' and len(context_window) > 0:\n",
        "                    # This is User A's response - create training pair\n",
        "                    context = ' [SEP] '.join(context_window)\n",
        "                    response = message\n",
        "                    training_pairs.append({\n",
        "                        'context': context,\n",
        "                        'response': response,\n",
        "                        'full_text': f\"{context} [RESPONSE] {response}\"\n",
        "                    })\n",
        "\n",
        "                # Add current message to context\n",
        "                context_window.append(f\"{sender}: {message}\")\n",
        "\n",
        "                # Keep only last 5 messages for context (adjustable)\n",
        "                if len(context_window) > 5:\n",
        "                    context_window.pop(0)\n",
        "\n",
        "        return training_pairs\n",
        "\n",
        "    def separate_user_conversations(self, conversations):\n",
        "        \"\"\"Separate conversations by user for analysis.\"\"\"\n",
        "        user_a_msgs = []\n",
        "        user_b_msgs = []\n",
        "\n",
        "        for conv in conversations:\n",
        "            for msg in conv['messages']:\n",
        "                clean_msg = self.clean_message(msg['message'])\n",
        "                if msg['sender'] == 'User A':\n",
        "                    user_a_msgs.append(clean_msg)\n",
        "                elif msg['sender'] == 'User B':\n",
        "                    user_b_msgs.append(clean_msg)\n",
        "\n",
        "        self.user_a_conversations = user_a_msgs\n",
        "        self.user_b_conversations = user_b_msgs\n",
        "\n",
        "        return user_a_msgs, user_b_msgs\n",
        "\n",
        "    def save_separated_data(self, output_dir='./'):\n",
        "        \"\"\"Save separated user conversations to files.\"\"\"\n",
        "        pd.DataFrame({'messages': self.user_a_conversations}).to_csv(\n",
        "            f'{output_dir}user_a_conversations.csv', index=False\n",
        "        )\n",
        "        pd.DataFrame({'messages': self.user_b_conversations}).to_csv(\n",
        "            f'{output_dir}user_b_conversations.csv', index=False\n",
        "        )\n",
        "        print(f\"Saved User A conversations: {len(self.user_a_conversations)} messages\")\n",
        "        print(f\"Saved User B conversations: {len(self.user_b_conversations)} messages\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: CUSTOM DATASET FOR TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "class ConversationDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for conversation pairs.\"\"\"\n",
        "\n",
        "    def __init__(self, data_pairs, tokenizer, max_length=256):\n",
        "        self.data_pairs = data_pairs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.data_pairs[idx]\n",
        "\n",
        "        # Encode the full text (context + response)\n",
        "        encoding = self.tokenizer(\n",
        "            pair['full_text'],\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        # For language modeling, labels are the same as input_ids\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: MODEL TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "class ConversationModelTrainer:\n",
        "    \"\"\"Handles model fine-tuning and training.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='gpt2', device=None):\n",
        "        \"\"\"\n",
        "        Initialize trainer with model.\n",
        "\n",
        "        Args:\n",
        "            model_name: Pretrained model to use ('gpt2', 'gpt2-medium', etc.)\n",
        "            device: Training device (cuda/cpu)\n",
        "        \"\"\"\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Load tokenizer and model\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        # Add special tokens\n",
        "        special_tokens = {'additional_special_tokens': ['[SEP]', '[RESPONSE]']}\n",
        "        self.tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        print(f\"Model loaded: {model_name}\")\n",
        "        print(f\"Vocabulary size: {len(self.tokenizer)}\")\n",
        "\n",
        "    def train(self, train_dataset, val_dataset=None,\n",
        "              epochs=5, batch_size=4, learning_rate=5e-5,\n",
        "              warmup_steps=100, save_steps=500, output_dir='./model_checkpoints'):\n",
        "        \"\"\"\n",
        "        Train the model.\n",
        "\n",
        "        Args:\n",
        "            train_dataset: Training dataset\n",
        "            val_dataset: Validation dataset (optional)\n",
        "            epochs: Number of training epochs\n",
        "            batch_size: Training batch size\n",
        "            learning_rate: Learning rate\n",
        "            warmup_steps: Warmup steps for scheduler\n",
        "            save_steps: Save checkpoint every N steps\n",
        "            output_dir: Directory to save checkpoints\n",
        "        \"\"\"\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        # Optimizer and scheduler\n",
        "        optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n",
        "        total_steps = len(train_loader) * epochs\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=warmup_steps,\n",
        "            num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        # Training loop\n",
        "        self.model.train()\n",
        "        global_step = 0\n",
        "        training_stats = []\n",
        "\n",
        "        print(f\"\\nStarting training for {epochs} epochs...\")\n",
        "        print(f\"Total steps: {total_steps}\")\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "            for batch in progress_bar:\n",
        "                # Move batch to device\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                loss = outputs.loss\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                global_step += 1\n",
        "\n",
        "                # Update progress bar\n",
        "                progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "                # Save checkpoint\n",
        "                if global_step % save_steps == 0:\n",
        "                    checkpoint_path = f\"{output_dir}/checkpoint-{global_step}\"\n",
        "                    self.save_model(checkpoint_path)\n",
        "\n",
        "            avg_epoch_loss = epoch_loss / len(train_loader)\n",
        "            print(f\"Epoch {epoch+1} - Average Loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "            training_stats.append({\n",
        "                'epoch': epoch + 1,\n",
        "                'avg_loss': avg_epoch_loss\n",
        "            })\n",
        "\n",
        "            # Validation\n",
        "            if val_dataset:\n",
        "                val_loss = self.evaluate(val_dataset, batch_size)\n",
        "                print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "                training_stats[-1]['val_loss'] = val_loss\n",
        "\n",
        "        return training_stats\n",
        "\n",
        "    def evaluate(self, dataset, batch_size=4):\n",
        "        \"\"\"Evaluate model on dataset.\"\"\"\n",
        "        self.model.eval()\n",
        "        eval_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        total_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in eval_loader:\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                total_loss += outputs.loss.item()\n",
        "\n",
        "        self.model.train()\n",
        "        return total_loss / len(eval_loader)\n",
        "\n",
        "    def save_model(self, path):\n",
        "        \"\"\"Save model and tokenizer.\"\"\"\n",
        "        self.model.save_pretrained(path)\n",
        "        self.tokenizer.save_pretrained(path)\n",
        "        print(f\"Model saved to {path}\")\n",
        "\n",
        "    def load_model(self, path):\n",
        "        \"\"\"Load model and tokenizer.\"\"\"\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(path)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(path)\n",
        "        self.model.to(self.device)\n",
        "        print(f\"Model loaded from {path}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: RESPONSE GENERATION\n",
        "# ============================================================================\n",
        "\n",
        "class ResponseGenerator:\n",
        "    \"\"\"Generates responses using trained model.\"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, device):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "\n",
        "    def generate_response(self, context, max_length=100,\n",
        "                         num_return_sequences=1,\n",
        "                         temperature=0.7,\n",
        "                         top_k=50,\n",
        "                         top_p=0.95,\n",
        "                         do_sample=True):\n",
        "        \"\"\"\n",
        "        Generate response given conversation context.\n",
        "\n",
        "        Args:\n",
        "            context: Conversation history\n",
        "            max_length: Maximum length of generated response\n",
        "            num_return_sequences: Number of responses to generate\n",
        "            temperature: Sampling temperature\n",
        "            top_k: Top-k sampling\n",
        "            top_p: Nucleus sampling\n",
        "            do_sample: Whether to use sampling\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        # Format input with response prompt\n",
        "        input_text = f\"{context} [RESPONSE]\"\n",
        "\n",
        "        # Encode input\n",
        "        input_ids = self.tokenizer.encode(\n",
        "            input_text,\n",
        "            return_tensors='pt'\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            output_sequences = self.model.generate(\n",
        "                input_ids=input_ids,\n",
        "                max_length=input_ids.shape[1] + max_length,\n",
        "                num_return_sequences=num_return_sequences,\n",
        "                temperature=temperature,\n",
        "                top_k=top_k,\n",
        "                top_p=top_p,\n",
        "                do_sample=do_sample,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode responses\n",
        "        responses = []\n",
        "        for sequence in output_sequences:\n",
        "            text = self.tokenizer.decode(sequence, skip_special_tokens=False)\n",
        "\n",
        "            # Extract only the response part\n",
        "            if '[RESPONSE]' in text:\n",
        "                response = text.split('[RESPONSE]')[-1].strip()\n",
        "                # Remove EOS token\n",
        "                response = response.replace(self.tokenizer.eos_token, '').strip()\n",
        "                responses.append(response)\n",
        "\n",
        "        return responses\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: EVALUATION METRICS\n",
        "# ============================================================================\n",
        "\n",
        "class ResponseEvaluator:\n",
        "    \"\"\"Evaluates generated responses using multiple metrics.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
        "            ['rouge1', 'rouge2', 'rougeL'],\n",
        "            use_stemmer=True\n",
        "        )\n",
        "        self.smoothing = SmoothingFunction()\n",
        "\n",
        "    def calculate_bleu(self, reference, hypothesis):\n",
        "        \"\"\"Calculate BLEU score.\"\"\"\n",
        "        reference_tokens = reference.split()\n",
        "        hypothesis_tokens = hypothesis.split()\n",
        "\n",
        "        # Calculate BLEU with smoothing\n",
        "        bleu_score = sentence_bleu(\n",
        "            [reference_tokens],\n",
        "            hypothesis_tokens,\n",
        "            smoothing_function=self.smoothing.method1\n",
        "        )\n",
        "\n",
        "        return bleu_score\n",
        "\n",
        "    def calculate_rouge(self, reference, hypothesis):\n",
        "        \"\"\"Calculate ROUGE scores.\"\"\"\n",
        "        scores = self.rouge_scorer.score(reference, hypothesis)\n",
        "\n",
        "        return {\n",
        "            'rouge1': scores['rouge1'].fmeasure,\n",
        "            'rouge2': scores['rouge2'].fmeasure,\n",
        "            'rougeL': scores['rougeL'].fmeasure\n",
        "        }\n",
        "\n",
        "    def calculate_perplexity(self, model, tokenizer, texts, device):\n",
        "        \"\"\"Calculate perplexity on a set of texts.\"\"\"\n",
        "        model.eval()\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for text in texts:\n",
        "                encodings = tokenizer(\n",
        "                    text,\n",
        "                    return_tensors='pt',\n",
        "                    truncation=True,\n",
        "                    max_length=512\n",
        "                )\n",
        "\n",
        "                input_ids = encodings['input_ids'].to(device)\n",
        "\n",
        "                outputs = model(input_ids, labels=input_ids)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                total_loss += loss.item() * input_ids.shape[1]\n",
        "                total_tokens += input_ids.shape[1]\n",
        "\n",
        "        perplexity = math.exp(total_loss / total_tokens)\n",
        "        return perplexity\n",
        "\n",
        "    def evaluate_predictions(self, predictions, references):\n",
        "        \"\"\"\n",
        "        Evaluate a set of predictions against references.\n",
        "\n",
        "        Args:\n",
        "            predictions: List of predicted responses\n",
        "            references: List of reference responses\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing average metrics\n",
        "        \"\"\"\n",
        "        bleu_scores = []\n",
        "        rouge1_scores = []\n",
        "        rouge2_scores = []\n",
        "        rougeL_scores = []\n",
        "\n",
        "        for pred, ref in zip(predictions, references):\n",
        "            # BLEU\n",
        "            bleu = self.calculate_bleu(ref, pred)\n",
        "            bleu_scores.append(bleu)\n",
        "\n",
        "            # ROUGE\n",
        "            rouge = self.calculate_rouge(ref, pred)\n",
        "            rouge1_scores.append(rouge['rouge1'])\n",
        "            rouge2_scores.append(rouge['rouge2'])\n",
        "            rougeL_scores.append(rouge['rougeL'])\n",
        "\n",
        "        return {\n",
        "            'bleu': np.mean(bleu_scores),\n",
        "            'rouge1': np.mean(rouge1_scores),\n",
        "            'rouge2': np.mean(rouge2_scores),\n",
        "            'rougeL': np.mean(rougeL_scores)\n",
        "        }\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def main_pipeline(data_path):\n",
        "    \"\"\"\n",
        "    Complete pipeline for training and evaluating the conversation model.\n",
        "\n",
        "    Args:\n",
        "        data_path: Path to conversation data CSV/PDF\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"CONVERSATIONAL RESPONSE PREDICTION SYSTEM\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Step 1: Load and preprocess data\n",
        "    print(\"\\n[STEP 1] Loading and preprocessing data...\")\n",
        "\n",
        "    # Read the uploaded data\n",
        "    df = pd.read_csv(data_path)\n",
        "\n",
        "    processor = ConversationDataProcessor(df)\n",
        "    conversations = processor.load_and_parse_data()\n",
        "\n",
        "    print(f\"Loaded {len(conversations)} conversations\")\n",
        "\n",
        "    # Separate user conversations\n",
        "    user_a_msgs, user_b_msgs = processor.separate_user_conversations(conversations)\n",
        "    processor.save_separated_data()\n",
        "\n",
        "    # Create training pairs\n",
        "    training_pairs = processor.create_training_pairs(conversations)\n",
        "    print(f\"Created {len(training_pairs)} training pairs\")\n",
        "\n",
        "    # Split data\n",
        "    train_pairs, test_pairs = train_test_split(\n",
        "        training_pairs,\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_pairs, val_pairs = train_test_split(\n",
        "        train_pairs,\n",
        "        test_size=0.1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Train: {len(train_pairs)}, Val: {len(val_pairs)}, Test: {len(test_pairs)}\")\n",
        "\n",
        "    # Step 2: Initialize trainer and create datasets\n",
        "    print(\"\\n[STEP 2] Initializing model and creating datasets...\")\n",
        "\n",
        "    trainer = ConversationModelTrainer(model_name='gpt2')\n",
        "\n",
        "    train_dataset = ConversationDataset(train_pairs, trainer.tokenizer)\n",
        "    val_dataset = ConversationDataset(val_pairs, trainer.tokenizer)\n",
        "    test_dataset = ConversationDataset(test_pairs, trainer.tokenizer)\n",
        "\n",
        "    # Step 3: Train model\n",
        "    print(\"\\n[STEP 3] Training model...\")\n",
        "\n",
        "    training_stats = trainer.train(\n",
        "        train_dataset=train_dataset,\n",
        "        val_dataset=val_dataset,\n",
        "        epochs=3,  # Adjust based on your needs\n",
        "        batch_size=2,  # Adjust based on GPU memory\n",
        "        learning_rate=5e-5\n",
        "    )\n",
        "\n",
        "    # Save final model\n",
        "    trainer.save_model('./final_model')\n",
        "\n",
        "    # Step 4: Generate responses\n",
        "    print(\"\\n[STEP 4] Generating responses on test set...\")\n",
        "\n",
        "    generator = ResponseGenerator(trainer.model, trainer.tokenizer, trainer.device)\n",
        "\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    for pair in test_pairs[:10]:  # Test on first 10 examples\n",
        "        context = pair['context']\n",
        "        reference = pair['response']\n",
        "\n",
        "        generated = generator.generate_response(\n",
        "            context,\n",
        "            max_length=50,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        if generated:\n",
        "            predictions.append(generated[0])\n",
        "            references.append(reference)\n",
        "\n",
        "            print(f\"\\nContext: {context}\")\n",
        "            print(f\"Reference: {reference}\")\n",
        "            print(f\"Generated: {generated[0]}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "    # Step 5: Evaluate\n",
        "    print(\"\\n[STEP 5] Evaluating model...\")\n",
        "\n",
        "    evaluator = ResponseEvaluator()\n",
        "\n",
        "    metrics = evaluator.evaluate_predictions(predictions, references)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"BLEU Score: {metrics['bleu']:.4f}\")\n",
        "    print(f\"ROUGE-1: {metrics['rouge1']:.4f}\")\n",
        "    print(f\"ROUGE-2: {metrics['rouge2']:.4f}\")\n",
        "    print(f\"ROUGE-L: {metrics['rougeL']:.4f}\")\n",
        "\n",
        "    # Calculate perplexity\n",
        "    test_texts = [pair['full_text'] for pair in test_pairs]\n",
        "    perplexity = evaluator.calculate_perplexity(\n",
        "        trainer.model,\n",
        "        trainer.tokenizer,\n",
        "        test_texts,\n",
        "        trainer.device\n",
        "    )\n",
        "    print(f\"Perplexity: {perplexity:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    return trainer, generator, evaluator, metrics\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# USAGE EXAMPLE\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the complete pipeline\n",
        "    # Replace with your actual data path\n",
        "    data_path = \"conversationfile.csv\"  # or .pdf if exported as PDF\n",
        "\n",
        "    trainer, generator, evaluator, metrics = main_pipeline(data_path)\n",
        "\n",
        "    # Example: Generate response for new context\n",
        "    print(\"\\n\\n\" + \"=\" * 80)\n",
        "    print(\"INTERACTIVE EXAMPLE\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    new_context = \"User B: Hey, did you finish the report? [SEP] User A: Almost done, just need to add the charts.\"\n",
        "\n",
        "    responses = generator.generate_response(\n",
        "        new_context,\n",
        "        num_return_sequences=3,\n",
        "        temperature=0.8\n",
        "    )\n",
        "\n",
        "    print(f\"\\nContext: {new_context}\")\n",
        "    print(\"\\nGenerated Responses:\")\n",
        "    for i, response in enumerate(responses, 1):\n",
        "        print(f\"{i}. {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM_jZ0mi6nqX",
        "outputId": "db959e15-0b32-4b6e-8710-85af8a589567"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CONVERSATIONAL RESPONSE PREDICTION SYSTEM\n",
            "================================================================================\n",
            "\n",
            "[STEP 1] Loading and preprocessing data...\n",
            "Loaded 4 conversations\n",
            "Saved User A conversations: 11 messages\n",
            "Saved User B conversations: 11 messages\n",
            "Created 9 training pairs\n",
            "Train: 6, Val: 1, Test: 2\n",
            "\n",
            "[STEP 2] Initializing model and creating datasets...\n",
            "Using device: cuda\n",
            "Model loaded: gpt2\n",
            "Vocabulary size: 50259\n",
            "\n",
            "[STEP 3] Training model...\n",
            "\n",
            "Starting training for 3 epochs...\n",
            "Total steps: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.88it/s, loss=7.45]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Average Loss: 8.3912\n",
            "Validation Loss: 10.5800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.55it/s, loss=7.77]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Average Loss: 8.1086\n",
            "Validation Loss: 10.0677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.39it/s, loss=7.03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Average Loss: 7.6163\n",
            "Validation Loss: 9.0668\n",
            "Model saved to ./final_model\n",
            "\n",
            "[STEP 4] Generating responses on test set...\n",
            "\n",
            "Context: User A: Finally watched that new sci-fi movie everyone's talking about. [SEP] User B: Nice! What did you think? I loved the visuals.\n",
            "Reference: Visuals were amazing, but the plot was a bit predictable for me.\n",
            "Generated: User C: I thought the story was amazing. ð¼¤ð½¢ð½¢ð½¢ð½¢ð½¢ð½¢ð½¢ð½¢ï¿½\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Context: User B: Hey, did you see the client's feedback on the mockups? [SEP] User A: Just saw it. They want a lot of changes to the color scheme. [SEP] User B: Yeah, that's what I was thinking. It's a big shift from the original brief.\n",
            "Reference: I'll start on the revisions. Can you update the project timeline?\n",
            "Generated: User B: Yeah. ã…‹ ONSORED ã…‹ ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ ã…‹ ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ ã…‹ ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ ã…‹ã…‹ã…‹ã…‹\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[STEP 5] Evaluating model...\n",
            "\n",
            "================================================================================\n",
            "EVALUATION RESULTS\n",
            "================================================================================\n",
            "BLEU Score: 0.0102\n",
            "ROUGE-1: 0.1500\n",
            "ROUGE-2: 0.0000\n",
            "ROUGE-L: 0.1000\n",
            "Perplexity: 53.9670\n",
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETE\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "INTERACTIVE EXAMPLE\n",
            "================================================================================\n",
            "\n",
            "Context: User B: Hey, did you finish the report? [SEP] User A: Almost done, just need to add the charts.\n",
            "\n",
            "Generated Responses:\n",
            "1. User B: Hey, did you finish the report? Ü¬ÜªÜ´Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬Ü¬\n",
            "2. User B: Hey, did you finish the report? _. User B: Yeah, it's been done too! _. User A: No, no, no, no, no, no! _. User A: Yeah, it's been done too! _. User B: Yeah, it's been done too! _. User A: Yeah, it's been done too! _. User B: Yeah, it's been done too! _. User A: Yeah,\n",
            "3. User B: What is that? _. User A: Looks like you have a lot of work to do. _. User A: Yeah, I mean, I've been trying to put out this since last week. _. User A: I'll try to finish it later. _. User B: You should have gotten this done as soon as possible. _. User A: We've been trying for a long time now. _. User B: You're going to need some\n"
          ]
        }
      ]
    }
  ]
}